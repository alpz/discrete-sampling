{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c382dc7a-e8e6-4999-8fdc-4e642bcf6795",
   "metadata": {},
   "source": [
    "# Graph Sampling for Neural Relational Inference\n",
    "\n",
    "In this tutorials we show an example of a variational autoencoder model with graph structured latent spaces.\n",
    "This model is called Neural Relational Inference and is described in this [paper]().\n",
    "This tutorial uses code from the associated code base available [here](https://github.com/ethanfetaya/NRI)\n",
    "\n",
    "The problem dealt by this model is one of predicting particle trajectories.\n",
    "Suppose that we have $N$ interacting particles (say, charges) with some interaction structure (say, attractive/repulsive forces) that are moving about in space.\n",
    "For each particle we are observe its trajectory (say, position and velocity) as it moves about over some period of time T.\n",
    "Each new state in the tracjectory of a particle will depend on the current state (position and velocity) and on the interaction with other particles.\n",
    "Our data consists of a set of $N$ particle trajectories but the actual interactions are unknown.\n",
    "The task of our model is to learn the dynamics of the particles to predict future trajectories given example trajectories only.\n",
    "\n",
    "Notice that the task of predicting particle dynamics would become easier if we knew the form of the interactions between particles. which we think of as a graph of interactions.\n",
    "Each particle in the system would occupy a node in the graph and the strenght of the interaction could be represented by the weight of the graph.\n",
    "The interaction graph could be fed to a neural network alongwith the currently known trajectory which could then predict the next steps of the particles.\n",
    "\n",
    "However, since in this problem we are not given the interaction graph, the approach taken by Neural Relational Inference is to use the encoder of a variational autoencoder to sample a graph using the given trajectory as input.\n",
    "For this the method uses a graph neural network as encoder.\n",
    "\n",
    "## Graph Network Encoder\n",
    "Since graph neural networks already require a graph over which to pass messages, the encoder starts with a fully connected graph with each node representing a particle.\n",
    "The entire trajectory information for a particle is used as the input feature for each node.\n",
    "Node features are transformed and passed over edges and concatenated to form edge features.\n",
    "This process can be repeated to get a deeper network with edge features as the output.\n",
    "\n",
    "The edge features are then transformed into edge weights (as (unnormalized) log probabilities).\n",
    "Finally from these edge weights we can sample an interaction graph by sample edges according to their weights. \n",
    "This interaction graph then serves as a latent variable $z$ to be used in the decoder where $z_{ij}$ indicates whether edge $(i,j)$ is present in the graph.\n",
    "Since we need to backpropagate into the encoder we relax the edge sampling operation using the Gumbel-Softmax relaxation.\n",
    "\n",
    "## Graph Network Decoder\n",
    "In the decoder we use another graph network which uses the graph sampled by the encoder and the input trajectory to predict the next step in the trajectory for some predefined number of steps.\n",
    "The models uses a gaussian likelihood loss for the trajectory which is fed into the VAE loss for optimization.\n",
    "\n",
    "The model structure can be seen in the following figure from the paper linked above.\n",
    "\n",
    "![nri](img/nri.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa553a-aac7-446e-abed-fae7fed066e0",
   "metadata": {},
   "source": [
    "#Neural Relational Inference\n",
    "\n",
    "Now we build a model to learn the dynamics of a 5 particle system connected by springs.\n",
    "The trajectory data for this system has been generated synthetically using the dynamical equations of motion and a ground truth interaction graph.\n",
    "The code to load the data is given in the `load_data` function in `utils.py`.\n",
    "\n",
    "As the first step we begin with some required imports and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f4e685d-7daf-4982-89dd-57d3a21d2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import math\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import networkx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220df0c8-88c0-4658-86b4-40dab913212b",
   "metadata": {},
   "source": [
    "### Loading and Examining Data\n",
    "\n",
    "We specify the batch size and the data file suffix to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a73a9f5-5e93-4195-a929-eafdf8cdbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader, _, _, _, _ = load_data(128, \"_springs5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc632bb1-ccc5-49d1-964f-3167f42f71f7",
   "metadata": {},
   "source": [
    "Let's now examine this data. We get an iterator from the data loader and retrieve the first minibatch.\n",
    "The dataset is in the form of tuples of trajectory information and the ground truth interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32dfe74b-f252-47e8-88b7-36b7bb7479d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5, 49, 4])\n",
      "torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "(x_sample,rel_sample) = next(iter(train_loader))\n",
    "print(x_sample.shape)\n",
    "print(rel_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1dc31-84db-45ef-b6a5-d62df9ea83d7",
   "metadata": {},
   "source": [
    "Let's look at the interaction graph first.\n",
    "This dataset consists of trajectories of systems of 5 particles. \n",
    "The interaction graph then specifies for each particle whether or not it interacts with every other particle.\n",
    "For 5 particles this gives us 20 interaction pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c426b3-695f-4614-8207-8203ed738a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print(rel_sample[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78430dc-49b7-47d9-abdf-d236b5f3f66b",
   "metadata": {},
   "source": [
    "This interaction is in the form of a list of interactions pairs. We can convert one such list to an interaction graph adjancency matrix as follows. \n",
    "Here we specify that a particle does not interact with itself by setting the diagonal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0024f5-88f5-43ff-ac3c-f15d596209ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros((5*5))\n",
    "for i in range(4):\n",
    "    b[i*5+i+1:(i+1)*5+(i+1)] = rel_sample[idx,i*5:(i+1)*5]\n",
    "    \n",
    "print(b.reshape((5,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ed2c5-afa3-4269-897e-06fca2690030",
   "metadata": {},
   "source": [
    "We can draw the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0be504da-6c73-4c38-8c28-dadf27637b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARYElEQVR4nO3dfWxV533A8d+1r/HlNbwmkJelKyQYSHHXVBkbBZNuaapo2TQ1VVs1ytZmSjvSaZuSae0iTfsnart2mvYSWq3q9kfSTZ1YVzUbUlN1wSSlNAppDXkhKUkIkJDEhoB58TW2790fFLeuXyHGvub3+Ugo5p7jw3OJ4Mtz7jnPKVSr1WoAQBJ1kz0AAJhIwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqwgdAKsIHQCrCB0AqxckeAOeu40R3bN55MPa83hmd5d6YUypG0+I58eHrr4wFsxone3gANa1QrVarkz0IxqbtwNF4YOveaH2hPSIiunsr/dtKxbqoRsSG5YtiY8uyaL5q7uQMEqDGTZnwZZ/lPLRjX9y/ZU+Ue/tipP9jhUJEqVgf993SFLeveceEjQ9gqqj58JnlnI3ec9HVUxl955+Z3lAX992yQvwAfklNh88s50z4P/q1HdHV0zfk9p4jr8ZrX/9MzGxaGwtvvXfAtukN9fHNu9bE6ivnTsBIAaaGmr2q8+eznJGjFxFRrUZ09fTF/Vuei4d27JuQ8U2UB7bujXLv0NGLiDjyyFejcck1Q24r9/bFpq17L9TQAKakmryqs+3A0bh/y55Bp/b6uo7H4S3/EOV9P4666XNiXssfxMxVG/q3d/VU4v4te2L1lXMvillOx4nuaH2hfdjwn3y2NepKM6NhQVP0Hj00aHu1GvHo8+1x+ER3is9BAcaiJmd8w81yjjzylSjUN8SVf/JQLLz13jj8yKY43f7KgH0uplnO5p0Hh91W6T4VRx/7Rsx7/50jHqMQEZufGv44ANnU3IxvuFlO5XQ5Tj2/PS7/oweibtr0KF21KmYs+/U4+cyjMW3DH/bvV0uznL6+viiXy9Hd3R3d3d3n/PX/HlkU3b3zhjz20W0PxqzmD0RxzqIRx1DurcSeQ8cvxNsDmJJqLnzDzXJ6j7wahbq6aJh/Rf9rDZf+anTv3z3k/g8+/tP46LsXvq3wjPb1aNsrlUqUSqVobGyMxsbGc/76VGnBmSnbLzn9xktRfqUtlnziH8b0e9pZ7hnTfgAZ1Fz49rzeOeCWhbMqPV1RaJwx4LW6xhlROd01aN/u3kp88V8eis//4N+GjMpYojNr1qxYsGDBeQXr7NfFYjEKhSHKNUZ/9s0fx7d/8tqg18v7d0fvsTfi4KZPRERE9XQ5olqJQx1/OmQM55QaznsMABebmgtfZ7l3yNfrGqZHtXtg5Krdp6Ju2vQh97/l9z4UX//WF8Z9fBOpafGcaCy+PugfArPefXPMXLG+/+edT3wreo+9EfNvvnvQMUrFumhaMvuCjxVgqqi5i1vmlIZucXH+FVGt9EXPkVf7Xzv95svRsOjqYY4z9Wc5t11/5ZCv1zWUon7WvP4fhYZSFIrTon7GJYP2rUbEbe8Z+jgAGdVc+M7McgYPq25aKWYs/404+tg3onK6HOWDz8apvT+KmatuHLTvxTLLWTirMVquXRSjnS2du+7jg25ejzhzY/+NyxdN+kU+ALWk5sI33CwnImL+BzZGtfd0HPynj0fHd74UCz6wMaYNMeO7mGY5d29YFqVi/Xl9b6lYHxs3LBvnEQFMbTW5ZNldDz4Z33vujVFXbBlKoRBx88rL4qu3v3f8BzZJrNUJMH5qbsYXYZbzy25f846475YVMb2hftTTnoXCmTU6RQ9gaDU544swyxnKroNHY9PWvfHo8+1RiDM3p5919kkVNy5fFBs3LLsolmwDuBBqNnwRns4wnMMnumPzUwdjz6Hj0VnuiTmlhmhaMjtue0+OZxMCvB01Hb4IsxwAxlfNh+8ssxwAxsOUCR8AjIeavKoTAC4U4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIJXiZA8AgKmn40R3bN55MPa83hmd5d6YUypG0+I58eHrr4wFsxone3gjKlSr1epkDwKAqaHtwNF4YOveaH2hPSIiunsr/dtKxbqoRsSG5YtiY8uyaL5q7uQMchTCB8CYPLRjX9y/ZU+Ue/tipHIUChGlYn3cd0tT3L7mHRM2vrFyqhOAUZ2J3nPR1VMZdd9qNaKrpy/u3/JcRETNxc+MD4ARtR04Gh/92o7o6ukb8HrHw1+O8r62qPSUo37mvJiz5kMxu/nmAftMb6iPb961JlZfOXcCRzwy4QNgRHc9+GR877k3Bp3ePN3+SjTMuzwKxYboOXwgXv/3z8WlH/6baFy8rH+fQiHi5pWXxVdvf+8Ej3p4bmcAYFgdJ7qj9YX2IT/Tm7bo6igUG372s0IUohC9bx0asE+1GvHo8+1x+ET3hR/sGPmMD4Bhbd55cMTth7+7KU7u/n5Ue7tj2mVLY/rSwTO7QkRsfupgfGr90gs0ynMjfAAMa8/rnQNuWfhlC27eGPNv+lR0v7onyvt3R6G+YdA+5d5K7Dl0/EIO85w41QnAsI519Yy6T6GuPkpXrYq+4x1x/Mdbhtynszz6cSaKGR8AERFx7Nix2LVrV7S1tfX/95XLN0Spaf3YDlCpDPqM76w5pcEzwckifADJVCqVeOmllwYErq2tLdrb22PVqlXR3Nwczc3Ncccdd8STJ+fFVx7fP+h0Z9/Jo1F+pS2mL7shCsVpUd73kzj5XGssvPUvBv16pWJdNC2ZPVFvb1RuZwC4iB0/fjx27949IHK7d++O+fPnx+rVq/sj19zcHEuXLo36+voB399xojvWfvH/Bofv1LFo/+/Px+k3X46oVqJ4yaUx+/pbY/a7PzhoDI3Futj+l++vmTU8hQ9gCFNtEeZqtRr79u3rn72djdyhQ4di5cqVAyK3evXqmDdv3piPPdx9fGNRi/fxCR/AL5gKizCfPHkynn766QGR27VrV8yePbs/bGcjd80110Sx+PY+1Rpu5ZaxsHILQA2rtUWYq9VqHDhwoD9wZyN34MCBaGpqGjSLW7hw4QUby7ms1XnW9Ia6uO+WFdbqBKhFk/0Xe1dXVzzzzDODIlcqlQbErbm5OZYvXx4NDRN/lWSt/cPgfAkfkN5Qp/KqvT1x+JFNUd73k6iUT0Rx7pKY13LHoJVJzvVUXrVajddee21Q4F5++eW49tprB0Xu0ksvHc+3+rbtOng0Nm3dG48+3x6FOHNz+llnTwXfuHxRbNywrKZOb/4i4QPSG+rijcrpcnT+6L9i1rt+O+ovWRRdLz4ZHd/5Ulz+yX+O4tzL+vcb6eKN7u7uePbZZwdFrq6ublDgVqxYEdOmTZuItzsuDp/ojs1PHYw9h45HZ7kn5pQaomnJ7LjtPbV58c8vEj4gteEu1x/Ka1//TFyy9mMxs2ntgNcbi3Xx7U9cF/t/OjByL774YixdunRQ5BYvXhyFQuFCvSVG4QZ2ILXRFmE+q+/kW9Fz5NWYtuhXBm0rl7vixjs/FysLr0Vzc3PcdNNNcc8998TKlSujVCqN95B5m4QPSG20RZgjIqp9vdHxnS/HrHf9VjQsuGrQ9kKxMT726Xvi7z/yaxdqmIwji1QDqXWWe0fcXq1WouN//i6ivhjzb/r0eR+H2iF8QFrHjx+PE0feHHZ7tVqNw1v+MfpOHo1Fv/9XUagf/iRZLS3CzMiED0jjrbfeiocffjjuvffeuOGGG2LJkiXxwhNboz6GXpHkyHcfiJ7DB+LS2/466hqGv1Kx1hZhZmSu6gQuWu3t7fHYY49Fa2trtLa2xosvvhhr1qyJ9evXR0tLS9xwww1xorcw5FWdvcfejFe/8smI+oYo1P184eb5H7w7Zq26ccC+tbYIMyMTPuCicejQoWhtbY1t27ZFa2trHDx4MNauXRstLS2xfv36uP7664e8V+5iW4SZkbmqE5iy9u/f3z+b27ZtW3R0dMS6deuipaUl7rzzzmhubh7TAs13b1gWj/2047wWYS4V62PjhmXnM3wmiRkfMCVUq9V46aWX+kPX2toap06d6p/NtbS0xHXXXRd1ded36cJkr9XJxBE+oCZVq9XYs2dP/2nLbdu2RbVajZaWlv7YNTU1jesKKBfLIsyMTPiAiJj8B69WKpV4+umn+yO3bdu2mD59en/oWlpa4p3vfOcFX+rrYliEmZEJHyQ3WQ9e7e3tjba2tv7Tlo8//ngsWLCgfza3fv36uPrqq8ft1ztXU3kRZkYmfJDYRJ7a6+npiZ07d/aHbvv27XHFFVf0z+bWrVsXl19++fm9ETgHwgdJXeiLOcrlcjzxxBP9n9Ht2LEjli5dOiB0ixYtehvvAM6P8EFCQz14NSKic+fDcXL39+N0+76YuaIlFv7Onw/63uEevHrq1Kn44Q9/2P8Z3ZNPPhkrV67sv+Lyfe97X8ybN+9Cvi0YE/fxQUIPbN0b5d7B96wVZy2IS37zI9H18lNR7Tk95PeWe/ti09a98be/e21s3769/9Tlrl27orm5OVpaWuKzn/1srF27NmbPtowXtceMD5IZy4NX39r2YPR1dgw544uIiL6eOPyvfxzvfVdT/4xuzZo1MWPGjAs0ahg/ZnyQzFgfvDqShoaG+MJ/bo273798HEYEE8vTGSCZsTx4dTQ9lYi97V3jNCKYWMIHyYzXA1M7yz3jchyYaMIHycwpjc8nHB68ylQlfJBM0+I50Vgc+o9+tdIX1d7TEZW+iGolqr2no1oZfPWnB68ylbmqE5IZ6arOo499I4794D8GvHbJ2o/F3HUfH/CaB68ylQkfJOTBq2TmVCckdPeGZVEq1p/X93rwKlOd8EFCzVfNjftuaYrpDef2V8CZtTqbPI6HKc0N7JDU2YWmPXiVbHzGB8l58CrZCB8QER68Sh7CB0AqLm4BIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMgFeEDIBXhAyAV4QMglf8H2YgOTcrIA9sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = b.reshape((5,5)).cpu().numpy()\n",
    "graph = networkx.from_numpy_array(g)\n",
    "networkx.draw(graph, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deaeca4-4231-4692-a591-1f68c10ee5f3",
   "metadata": {},
   "source": [
    "Let's now examine the trajectory data for the first data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec7c65ea-79fe-4ec8-8d1d-7d88b619404a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 49, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample[idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c006d5-a5cd-443f-afba-186b71887c6e",
   "metadata": {},
   "source": [
    "The shape of the data above specifies that each entry consists of 5 particles with trajectories given for 49 time steps.\n",
    "Furthermore, each state in the trajectory is specified by a 4 dimensional vector.\n",
    "In this case the state is a 2d position and 2d velocity pair specifying the position and velocity of each particle at each time step.\n",
    "We examine the position and velocity of the first particle in the first trajectory for a couple of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fba25cfc-42a6-4b3d-b0b1-43710d9c3b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1272, -0.0987, -0.3305,  0.1197],\n",
       "        [-0.1380, -0.0945, -0.3275,  0.1196]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample[idx,0,0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "718c3f7e-ad1f-44d2-bad7-68c20e23aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=4\n",
    "num_atoms=5\n",
    "timesteps=49\n",
    "lr=0.0005\n",
    "temp=0.5\n",
    "output_var=5e-5\n",
    "\n",
    "_EPS = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6656d05c-c319-4eac-b6c8-423350867781",
   "metadata": {},
   "source": [
    "Recall that we mentioned above that the encoder of the model works on the fully connected graph in order to predict the interaction graph.\n",
    "To pass messages over the fully connected graph, it is useful to define some relational masks specifying which vertices receive messages from which other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7433889d-7c24-48c4-81a4-2edf202e354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "off_diag = np.ones([num_atoms, num_atoms]) - np.eye(num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cde3af-eb71-49a8-b244-9693f875416d",
   "metadata": {},
   "source": [
    "We can convert edge features into node features and node features into edge features by passing messages over the fully connected graph using these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "603dcfd2-c5f8-4b9b-a154-b85f56c2826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 1.]]) torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "print(rel_rec.t(), rel_rec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03fa73-14bc-4625-88c1-c7f7d1944172",
   "metadata": {},
   "source": [
    "For example to convert edge features for the 20 interactions into node features we can multiply the above matrix with the edge feature vector as \n",
    "```\n",
    "torch.matmul(rel_rec.t(), x)\n",
    "```\n",
    "which collects the messages from all neighboring nodes for each vertex and adds the messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f328c-bf0c-40be-9a8d-9fc1fdbe6638",
   "metadata": {},
   "source": [
    "Next we define a simple MLP class to be used for the nonlinear feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf9717f-99fd-4284-a2d9-4801cf6e878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Two-layer fully-connected ELU net with batch norm.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def batch_norm(self, inputs):\n",
    "        x = inputs.view(inputs.size(0) * inputs.size(1), -1)\n",
    "        x = self.bn(x)\n",
    "        return x.view(inputs.size(0), inputs.size(1), -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input shape: [num_sims, num_things, num_features]\n",
    "        x = F.elu(self.fc1(inputs))\n",
    "        x = F.dropout(x, self.dropout_prob, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.batch_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44406999-5158-4484-a56f-6dfd423dd6fd",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Next we specify the VAE encoder as a graph neural network.\n",
    "For each particle we construct a single feature vector by using the entire trajectory information.\n",
    "We include three graph neural network layers that convert the node features into edge features into node features and finally into edge features. \n",
    "The final edge features are then used to parameterize the edge probabilities for the graph sampling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a127c4-0690-4358-b097-ac7f5c0c2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_out=2, do_prob=0.,):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n",
    "        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n",
    "        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n",
    "        self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n",
    "        \n",
    "        self.fc_out = nn.Linear(n_hid, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def edge2node(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        incoming = torch.matmul(rel_rec.t(), x)\n",
    "        return incoming / incoming.size(1)\n",
    "\n",
    "    def node2edge(self, x, rel_rec, rel_send):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "        receivers = torch.matmul(rel_rec, x)\n",
    "        senders = torch.matmul(rel_send, x)\n",
    "        edges = torch.cat([senders, receivers], dim=2)\n",
    "        return edges\n",
    "\n",
    "    def forward(self, inputs, rel_rec, rel_send):\n",
    "        # Input shape: [num_sims, num_atoms, num_timesteps, num_dims]\n",
    "        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n",
    "        # New shape: [num_sims, num_atoms, num_timesteps*num_dims]\n",
    "\n",
    "        x = self.mlp1(x)  # 2-layer ELU net per node\n",
    "\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = self.mlp2(x)\n",
    "        x_skip = x\n",
    "\n",
    "        x = self.edge2node(x, rel_rec, rel_send)\n",
    "        x = self.mlp3(x)\n",
    "        x = self.node2edge(x, rel_rec, rel_send)\n",
    "        x = torch.cat((x, x_skip), dim=2)  # Skip connection\n",
    "        x = self.mlp4(x)\n",
    "\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b92015-2ce8-45c6-bf59-c7a99120171d",
   "metadata": {},
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448792ed-f196-464d-81db-e3edab31a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"MLP decoder module.\"\"\"\n",
    "\n",
    "    def __init__(self, n_in_node, edge_types, msg_hid, msg_out, n_hid,\n",
    "                 do_prob=0.):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "\n",
    "        self.msg_fc1 = (nn.Linear(2 * n_in_node, msg_hid))\n",
    "        self.msg_fc2 = (nn.Linear(msg_hid, msg_out))\n",
    "        self.msg_out_shape = msg_out\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_in_node + msg_out, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "\n",
    "        print('Using learned interaction net decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "\n",
    "    def single_step_forward(self, single_timestep_inputs, rel_rec, rel_send,\n",
    "                            single_timestep_rel_type):\n",
    "\n",
    "        # single_timestep_inputs has shape\n",
    "        # [batch_size, num_timesteps, num_atoms, num_dims]\n",
    "\n",
    "        # single_timestep_rel_type has shape:\n",
    "        # [batch_size, num_timesteps, num_atoms*(num_atoms-1), num_edge_types]\n",
    "\n",
    "        # Node2edge\n",
    "        receivers = torch.matmul(rel_rec, single_timestep_inputs)\n",
    "        senders = torch.matmul(rel_send, single_timestep_inputs)\n",
    "        pre_msg = torch.cat([senders, receivers], dim=-1)\n",
    "\n",
    "        msg = F.relu(self.msg_fc1(pre_msg))\n",
    "        msg = F.dropout(msg, p=self.dropout_prob)\n",
    "        msg = F.relu(self.msg_fc2(msg))\n",
    "        msg = msg * single_timestep_rel_type[:, :, :, 1:2]\n",
    "\n",
    "        # Aggregate all msgs to receiver\n",
    "        agg_msgs = msg.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)\n",
    "        agg_msgs = agg_msgs.contiguous()\n",
    "\n",
    "        # Skip connection\n",
    "        aug_inputs = torch.cat([single_timestep_inputs, agg_msgs], dim=-1)\n",
    "\n",
    "        # Output MLP\n",
    "        pred = F.dropout(F.relu(self.out_fc1(aug_inputs)), p=self.dropout_prob)\n",
    "        pred = F.dropout(F.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "\n",
    "        # Predict position/velocity difference\n",
    "        return single_timestep_inputs + pred\n",
    "\n",
    "    def forward(self, inputs, rel_type, rel_rec, rel_send, pred_steps=1):\n",
    "        # NOTE: Assumes that we have the same graph across all samples.\n",
    "\n",
    "        inputs = inputs.transpose(1, 2).contiguous()\n",
    "\n",
    "        sizes = [rel_type.size(0), inputs.size(1), rel_type.size(1),\n",
    "                 rel_type.size(2)]\n",
    "        rel_type = rel_type.unsqueeze(1).expand(sizes)\n",
    "\n",
    "        time_steps = inputs.size(1)\n",
    "        assert (pred_steps <= time_steps)\n",
    "        preds = []\n",
    "\n",
    "        # initial step\n",
    "        last_pred = inputs[:, 0:1, :, :]\n",
    "        # NOTE: Assumes rel_type is constant (i.e. same across all time steps).\n",
    "        curr_rel_type = rel_type[:, 0:1, :, :]\n",
    "\n",
    "        # Run n prediction steps\n",
    "        for step in range(0, pred_steps):\n",
    "            last_pred = self.single_step_forward(last_pred, rel_rec, rel_send,\n",
    "                                                 curr_rel_type)\n",
    "            preds.append(last_pred)\n",
    "\n",
    "        sizes = [preds[0].size(0), preds[0].size(1) * pred_steps,\n",
    "                 preds[0].size(2), preds[0].size(3)]\n",
    "\n",
    "        output = Variable(torch.zeros(sizes))\n",
    "        if inputs.is_cuda:\n",
    "            output = output.cuda()\n",
    "\n",
    "        # Re-assemble correct timeline\n",
    "        for i in range(len(preds)):\n",
    "            output[:, i:i+1, :, :] = preds[i]\n",
    "\n",
    "        pred_all = output[:, :(inputs.size(1) - 1), :, :]\n",
    "\n",
    "        return pred_all.transpose(1, 2).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddcd409-80ac-452a-89e9-f91e1caba245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate off-diagonal interaction graph\n",
    "#off_diag = np.ones([args.num_atoms, args.num_atoms]) - np.eye(args.num_atoms)\n",
    "off_diag = np.ones([num_atoms, num_atoms]) - np.eye(num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4634d87c-9724-440b-a12e-4b672883189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using factor graph MLP encoder.\n",
      "Using learned interaction net decoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  app.launch_new_instance()\n",
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n"
     ]
    }
   ],
   "source": [
    "encoder = MLPEncoder(timesteps * dims, 256, 2)\n",
    "\n",
    "decoder = MLPDecoder(n_in_node=dims,\n",
    "                         edge_types=2,\n",
    "                         msg_hid=256,\n",
    "                         msg_out=256,\n",
    "                         n_hid=256,\n",
    "                         #do_prob=args.decoder_dropout,\n",
    "                         #skip_first=args.skip_first\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedba580-7db9-4123-89ee-b6794114a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "rel_rec = rel_rec.cuda()\n",
    "rel_send = rel_send.cuda()\n",
    "    #triu_indices = triu_indices.cuda()\n",
    "    #tril_indices = tril_indices.cuda()\n",
    "\n",
    "rel_rec = Variable(rel_rec)\n",
    "rel_send = Variable(rel_send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d039d23a-9a0e-4f8f-a2f3-90ced16b8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),\n",
    "                       lr=lr)\n",
    "train_loader, valid_loader, test_loader, loc_max, loc_min, vel_max, vel_min = load_data(\n",
    "    128, \"_springs5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e242e8-6bf6-4b24-ab19-25e42f5683b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, best_val_loss):\n",
    "    t = time.time()\n",
    "    nll_train = []\n",
    "    acc_train = []\n",
    "    kl_train = []\n",
    "    mse_train = []\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    #scheduler.step()\n",
    "    for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "        #if args.cuda:\n",
    "        data, relations = data.cuda(), relations.cuda()\n",
    "        data, relations = Variable(data), Variable(relations)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=temp, hard=False)\n",
    "        prob = my_softmax(logits, -1)\n",
    "\n",
    "        output = decoder(data, edges, rel_rec, rel_send, timesteps)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "\n",
    "        loss_nll = nll_gaussian(output, target, output_var)\n",
    "\n",
    "        loss_kl = kl_categorical_uniform(prob, num_atoms, 2)\n",
    "\n",
    "        loss = loss_nll + loss_kl\n",
    "\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_train.append(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mse_train.append(F.mse_loss(output, target).item())\n",
    "        nll_train.append(loss_nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "\n",
    "    nll_val = []\n",
    "    acc_val = []\n",
    "    kl_val = []\n",
    "    mse_val = []\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch_idx, (data, relations) in enumerate(valid_loader):\n",
    "        #if args.cuda:\n",
    "        data, relations = data.cuda(), relations.cuda()\n",
    "        data, relations = Variable(data, volatile=True), Variable(\n",
    "            relations, volatile=True)\n",
    "\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=temp, hard=True)\n",
    "        prob = my_softmax(logits, -1)\n",
    "\n",
    "        # validation output uses teacher forcing\n",
    "        output = decoder(data, edges, rel_rec, rel_send, timesteps)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "        loss_nll = nll_gaussian(output, target, output_var)\n",
    "\n",
    "        loss_kl = kl_categorical_uniform(prob, num_atoms, 2)\n",
    "\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_val.append(acc)\n",
    "\n",
    "        mse_val.append(F.mse_loss(output, target).item())\n",
    "        nll_val.append(loss_nll.item())\n",
    "        kl_val.append(loss_kl.item())\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "          'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "          'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "          'acc_train: {:.10f}'.format(np.mean(acc_train)),\n",
    "          'nll_val: {:.10f}'.format(np.mean(nll_val)),\n",
    "          'kl_val: {:.10f}'.format(np.mean(kl_val)),\n",
    "          'mse_val: {:.10f}'.format(np.mean(mse_val)),\n",
    "          'acc_val: {:.10f}'.format(np.mean(acc_val)),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    return np.mean(nll_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c689471d-3244-45d3-8935-d73b83e76c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apervez/work/course/ssampling-dl2/utils.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  soft_max_1d = F.softmax(trans_input)\n",
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/apervez/anaconda3/envs/pytorch1.9_2/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 nll_train: 80233.9459743646 kl_train: -0.4496140164 mse_train: 0.0417885140 acc_train: 0.5068346387 nll_val: 14074.0770495451 kl_val: -0.5037677816 mse_val: 0.0073302486 acc_val: 0.5165595332 time: 42.2006s\n",
      "Epoch: 0001 nll_train: 11736.1058433903 kl_train: -0.7378520523 mse_train: 0.0061125554 acc_train: 0.5460475943 nll_val: 10734.5435250198 kl_val: -1.0583995732 mse_val: 0.0055909082 acc_val: 0.5925534019 time: 38.9282s\n",
      "Epoch: 0002 nll_train: 8400.7023482457 kl_train: -1.0399480785 mse_train: 0.0043753660 acc_train: 0.6344305467 nll_val: 8956.9209107991 kl_val: -1.0081337290 mse_val: 0.0046650631 acc_val: 0.6613825158 time: 38.8671s\n",
      "Epoch: 0003 nll_train: 7106.6398295137 kl_train: -0.9441673736 mse_train: 0.0037013751 acc_train: 0.6915630994 nll_val: 7781.2127484672 kl_val: -0.9332466872 mse_val: 0.0040527151 acc_val: 0.7158079509 time: 38.8326s\n",
      "Epoch: 0004 nll_train: 6255.6973667779 kl_train: -0.8646283267 mse_train: 0.0032581759 acc_train: 0.7474564418 nll_val: 6501.7719479331 kl_val: -0.8406009938 mse_val: 0.0033863396 acc_val: 0.7850326345 time: 38.7994s\n",
      "Epoch: 0005 nll_train: 4707.0161207741 kl_train: -0.7303188548 mse_train: 0.0024515710 acc_train: 0.8334137228 nll_val: 4258.2337074763 kl_val: -0.6298910840 mse_val: 0.0022178301 acc_val: 0.8790051424 time: 38.8592s\n",
      "Epoch: 0006 nll_train: 2920.3714877468 kl_train: -0.5307836576 mse_train: 0.0015210269 acc_train: 0.8955900336 nll_val: 3416.9176195362 kl_val: -0.4672076332 mse_val: 0.0017796446 acc_val: 0.9034266218 time: 38.8715s\n",
      "Epoch: 0007 nll_train: 2376.2965553069 kl_train: -0.4203724631 mse_train: 0.0012376545 acc_train: 0.9136097347 nll_val: 2705.0449311462 kl_val: -0.3845676697 mse_val: 0.0014088776 acc_val: 0.9176275712 time: 38.8563s\n",
      "Epoch: 0008 nll_train: 2137.2883550541 kl_train: -0.3493553543 mse_train: 0.0011131711 acc_train: 0.9231659607 nll_val: 2797.9604337668 kl_val: -0.3281093870 mse_val: 0.0014572711 acc_val: 0.9242039161 time: 38.8247s\n",
      "Epoch: 0009 nll_train: 1870.4181994660 kl_train: -0.2984925324 mse_train: 0.0009741762 acc_train: 0.9304503676 nll_val: 2033.3484164854 kl_val: -0.2857403423 mse_val: 0.0010590357 acc_val: 0.9321845332 time: 38.8115s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "for epoch in range(10):\n",
    "    val_loss = train(epoch, best_val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead91d0b-0338-41a0-bf53-ffc1432e53c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
